{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import operator\n",
    "import gym\n",
    "from skimage import io, color, transform\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,MaxPooling2D,Conv2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self,action_size,epsilon=1.0,experience_replay_capacity=1000,minibatch_size=32,learning_rate=0.01,gamma=0.95,preprocess_image_dim=84):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.experience_replay_capacity = experience_replay_capacity\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.preprocess_image_dim = preprocess_image_dim\n",
    "\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.log_path = []\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self.create_model()\n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (5,5),input_shape=(1,84,84), strides=(1,1), padding='same', data_format='channels_first', activation='relu', use_bias=True, bias_initializer='zeros'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid', data_format=None))\n",
    "\n",
    "        model.add(Conv2D(32, (5,5), strides=(1,1), padding='same', data_format=None, activation='relu', use_bias=True, bias_initializer='zeros'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid', data_format=None))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def append_experience_replay_example(self, experience_replay_example):\n",
    "        \"\"\"\n",
    "        Add an experience replay example to our agent's replay memory. If\n",
    "        memory is full, overwrite previous examples, starting with the oldest\n",
    "        \"\"\"\n",
    "        if (len(self.memory) < self.experience_replay_capacity):\n",
    "            self.memory.append(experience_replay_example)\n",
    "\n",
    "\n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "#         print(observation.shape)\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "#         print(grayscale_observation.shape) (210,160)\n",
    "        resized_observation = transform.resize(grayscale_observation, (1, self.preprocess_image_dim, self.preprocess_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation, 0)\n",
    "#         print(resized_observation.shape) (1,84,84)\n",
    "        return resized_observation\n",
    "\n",
    "    def take_action(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, the model attempts to take an action\n",
    "        according to its q-function approximation\n",
    "        \"\"\"\n",
    "        observation = np.array(observation)\n",
    "        observation = np.reshape(observation, [1,1,self.preprocess_image_dim,self.preprocess_image_dim])\n",
    "\n",
    "#         print(observation.shape) (1,84,84)\n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            action = random.randrange(self.action_size)\n",
    "            return action\n",
    "        act_values = self.model.predict(observation) # Forward Propagation\n",
    "        action = np.argmax(act_values[0])\n",
    "        self.log_path.append(action)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Allow the model to collect examples from its experience replay memory\n",
    "        and learn from them\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.memory, self.minibatch_size)\n",
    "        for obs, action, lives, reward, next_obs, done in minibatch:\n",
    "            obs = np.reshape(np.array(obs),[1,1,self.preprocess_image_dim,self.preprocess_image_dim])\n",
    "            next_obs = np.reshape(np.array(next_obs),[1,1,self.preprocess_image_dim,self.preprocess_image_dim])\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_obs)[0])\n",
    "            target_f = self.model.predict(obs)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(obs, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Hyperparameters\n",
    "#####\n",
    "\n",
    "GAME_TYPE = 'MsPacman-v0'\n",
    "\n",
    "#environment parameters\n",
    "NUM_EPISODES = 100\n",
    "MAX_TIMESTEPS = 1000\n",
    "FRAME_SKIP = 2\n",
    "PHI_LENGTH = 4\n",
    "\n",
    "#agent parameters\n",
    "NAIVE_RANDOM = False\n",
    "EPSILON = 1.0\n",
    "GAMMA = 0.95\n",
    "EXPERIENCE_REPLAY_CAPACITY = 1000\n",
    "MINIBATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "PREPROCESS_IMAGE_DIM = 84\n",
    "\n",
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Entry-point for running Ms. Pac-man simulation\n",
    "    \"\"\"\n",
    "\n",
    "    ENV = gym.make(GAME_TYPE)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    DONE = False\n",
    "\n",
    "    #print game parameters\n",
    "    print (\"~~~Environment Parameters~~~\")\n",
    "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
    "    print (\"Max timesteps: %s\" % MAX_TIMESTEPS)\n",
    "    print (\"Action space: %s\" % ACTION_SIZE)\n",
    "    print()\n",
    "    print (\"~~~Agent Parameters~~~\")\n",
    "    print (\"Naive Random: %s\" % NAIVE_RANDOM)\n",
    "    print (\"Epsilon: %s\" % EPSILON)\n",
    "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
    "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
    "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
    "\n",
    "    #initialize agent\n",
    "    agent = Agent(action_size = ACTION_SIZE,epsilon=EPSILON,\n",
    "                experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,\n",
    "                minibatch_size=MINIBATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,gamma = GAMMA,preprocess_image_dim=PREPROCESS_IMAGE_DIM)\n",
    "\n",
    "    #initialize auxiliary data structures\n",
    "#     S_LIST = [] # Stores PHI_LENGTH frames at a time\n",
    "#     TOT_FRAMES = 0  # Counter of frames covered till now\n",
    "\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        OBS = ENV.reset()\n",
    "        LOG_PATH = [] # Stores actions taken in each episode\n",
    "\n",
    "        for time in range(MAX_TIMESTEPS):\n",
    "            ENV.render()\n",
    "            OBS = agent.preprocess_observation(OBS)\n",
    "            # ensure that S_LIST is populated with PHI_LENGTH frames\n",
    "            \"\"\"\n",
    "            if TOT_FRAMES < PHI_LENGTH:\n",
    "                S_LIST.append(agent.preprocess_observation(OBS))\n",
    "                TOT_FRAMES += 1\n",
    "                continue\n",
    "            \"\"\"\n",
    "#             X = np.array(S_LIST)\n",
    "#             print(X.shape) #(4,1,84,84)\n",
    "            \n",
    "            # call take_action\n",
    "            ACTION = agent.take_action(OBS)\n",
    "#             print(ACTION)\n",
    "\n",
    "            NEXT_OBS, REWARD, DONE, LIVES = ENV.step(ACTION) # NEXT_OBS is a numpy.ndarray of shape(210,160,3)\n",
    "\n",
    "            LIVES = LIVES.get('ale.lives')\n",
    "        \n",
    "            # Calculation of Reward\n",
    "            REWARD = time*LIVES*LIVES \n",
    "            REWARD = REWARD if not DONE else -5\n",
    "                        \n",
    "#             if (time%50==0):\n",
    "#                 print(REWARD)\n",
    "\n",
    "#             print(NEXT_OBS, REWARD, DONE,'\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "            NEXT_OBS = agent.preprocess_observation(NEXT_OBS) # shape(1,84,84)\n",
    "            \n",
    "\n",
    "            EREG = [OBS, ACTION, LIVES, REWARD, NEXT_OBS, DONE]\n",
    "#             print(EREG)\n",
    "\n",
    "            agent.append_experience_replay_example(EREG)\n",
    "\n",
    "            OBS = NEXT_OBS\n",
    "            if DONE:\n",
    "                print(\"episode:{}/{}, score: {}, e = {}\".format(i_episode, NUM_EPISODES, REWARD, agent.epsilon))\n",
    "                break\n",
    "\n",
    "            #update state list with next observation\n",
    "            \"\"\"\n",
    "            S_LIST.append(agent.preprocess_observation(OBS))\n",
    "            S_LIST.pop(0)\n",
    "            \"\"\"\n",
    "\n",
    "        if (len(agent.memory)>agent.minibatch_size):\n",
    "            agent.learn()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-19 15:58:56,065] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Environment Parameters~~~\n",
      "Num episodes: 100\n",
      "Max timesteps: 1000\n",
      "Action space: 9\n",
      "\n",
      "~~~Agent Parameters~~~\n",
      "Naive Random: False\n",
      "Epsilon: 1.0\n",
      "Experience Replay Capacity: 1000\n",
      "Minibatch Size: 32\n",
      "Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajays/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0/100, score: -5, e = 1.0\n",
      "episode:1/100, score: -5, e = 0.995\n",
      "episode:2/100, score: -5, e = 0.990025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-f30736a3a2ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-8d35827ee39e>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_TIMESTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mENV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mOBS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# ensure that S_LIST is populated with PHI_LENGTH frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ajays/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ajays/gym/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ajays/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ajays/gym/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
